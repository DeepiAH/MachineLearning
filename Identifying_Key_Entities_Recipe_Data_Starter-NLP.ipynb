{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42UBKEnat_xo"
      },
      "source": [
        "# **Identifying Key Entities in Recipe Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pme3h_fduOKh"
      },
      "source": [
        "\n",
        "**Business Objective**:\n",
        "The goal of this assignment is to train a Named Entity Recognition (NER) model using Conditional Random Fields (CRF) to extract key entities from recipe data. The model will classify words into predefined categories such as ingredients, quantities and units, enabling the creation of a structured database of recipes and ingredients that can be used to power advanced features in recipe management systems, dietary tracking apps, or e-commerce platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXzoAs8evNG0"
      },
      "source": [
        "### **Data Description**\n",
        "The given data is in JSON format, representing a **structured recipe ingredient list** with **Named Entity Recognition (NER) labels**. Below is a breakdown of the data fields:\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"input\": \"6 Karela Bitter Gourd Pavakkai Salt 1 Onion 3 tablespoon Gram flour besan 2 teaspoons Turmeric powder Haldi Red Chilli Cumin seeds Jeera Coriander Powder Dhania Amchur Dry Mango Sunflower Oil\",\n",
        "        \"pos\": \"quantity ingredient ingredient ingredient ingredient ingredient quantity ingredient quantity unit ingredient ingredient ingredient quantity unit ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient\"\n",
        "    },\n",
        "    {\n",
        "      \"input\": \"2-1/2 cups rice cooked 3 tomatoes teaspoons BC Belle Bhat powder 1 teaspoon chickpea lentils 1/2 cumin seeds white urad dal mustard green chilli dry red 2 cashew or peanuts 1-1/2 tablespoon oil asafoetida\",\n",
        "      \"pos\": \"quantity unit ingredient ingredient quantity ingredient unit ingredient ingredient ingredient ingredient quantity unit ingredient ingredient quantity ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient ingredient quantity ingredient ingredient ingredient quantity unit ingredient ingredient\"\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSDcNvJlwC6N"
      },
      "source": [
        "| **Key**  | **Description**  |\n",
        "|----------|-----------------|\n",
        "| `input`  | Contains a raw ingredient list from a recipe. |\n",
        "| `pos`    | Represents the corresponding part-of-speech (POS) tags or NER labels, identifying quantities, ingredients, and units. |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phenosA4se1c"
      },
      "source": [
        "## **1** Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-jQHin3kQX"
      },
      "source": [
        "#### **1.1** Installation of sklearn-crfsuite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPhaJSfCwpfa"
      },
      "source": [
        "sklearn-crfsuite is a Python wrapper for CRFsuite, a fast and efficient implementation of Conditional Random Fields (CRFs). It is designed to integrate seamlessly with scikit-learn for structured prediction tasks such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_QawokgQXAMO"
      },
      "outputs": [],
      "source": [
        "#pip install --force-reinstall --no-deps matplotlib==3.10.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svqZwrHT3rzV"
      },
      "source": [
        "#### **1.2** Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0hlp-Ln4WsaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "593f0389-573b-456e-afc8-0fb28c271d5e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn_crfsuite'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1363944533.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#from matplotlib.backends import BackendFilter, backend_registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m  \u001b[0;31m# For advanced data visualisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn_crfsuite\u001b[0m  \u001b[0;31m# CRF (Conditional Random Fields) implementation for sequence modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m  \u001b[0;31m# For numerical computations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Saving and loading machine learning models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_crfsuite'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import json  # For handling JSON data\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import re  # For regular expressions (useful for text preprocessing)\n",
        "#from matplotlib import pyplot as plt  # For visualisation\n",
        "#from matplotlib.backends import BackendFilter, backend_registry\n",
        "import seaborn as sns  # For advanced data visualisation\n",
        "import sklearn_crfsuite  # CRF (Conditional Random Fields) implementation for sequence modeling\n",
        "import numpy as np  # For numerical computations\n",
        "# Saving and loading machine learning models\n",
        "import joblib\n",
        "import random\n",
        "import spacy\n",
        "from IPython.display import display, Markdown # For displaying well-formatted output\n",
        "from fractions import Fraction  # For handling fractional values in numerical data\n",
        "# Importing tools for feature engineering and model training\n",
        "from collections import Counter  # For counting occurrences of elements in a list\n",
        "from sklearn_crfsuite import metrics  # For evaluating CRF models\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "ELbeo9-A4AAO",
        "outputId": "3f39c244-3dfa-4da6-e227-e7c9b61f9a70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn_crfsuite'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3688613603.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn_crfsuite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_crfsuite'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "from sklearn_crfsuite import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2zLbaB0w1ZH"
      },
      "outputs": [],
      "source": [
        "# Import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3_LR6N_2cli"
      },
      "outputs": [],
      "source": [
        "# Ensure pandas displays full content\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUOu_u0fyMfh"
      },
      "source": [
        "## **2** Data Ingestion and Preparation <font color = red>[25 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ksMVNgeyiLN"
      },
      "source": [
        "#### **2.1** *Read Recipe Data from Dataframe and prepare the data for analysis* <font color = red>[12 marks]</font> <br>\n",
        "Read the data from JSON file, print first five rows and describe the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxn28jL3z4GY"
      },
      "source": [
        "##### **2.1.1** **Define a *load_json_dataframe* function** <font color = red>[7 marks]</font> <br>\n",
        "\n",
        "Define a function that takes path of the ingredient_and_quantity.json file and reads it, convert it into dataframe - df and return it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq6UgUYcPyOL"
      },
      "outputs": [],
      "source": [
        "# define a function to load json file to a dataframe\n",
        "filepath = \"C:\\\\Submission\\\\NLP\\\\ingredientandquantity.json\"\n",
        "def loadfile(filepath):\n",
        "  with open(filepath,'r') as f:\n",
        "    data=json.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NlhkH_605IA"
      },
      "source": [
        "##### **2.1.2** **Execute the *load_json_dataframe* function** <font color = red>[2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UONMkMsrxdxB"
      },
      "outputs": [],
      "source": [
        "# read the json file by giving the file path and create a dataframe\n",
        "df = pd.read_json(filepath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1VkDbev3UHP"
      },
      "source": [
        "##### **2.1.3** **Describe the dataframe** <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Print first five rows of dataframe along with dimensions. Display the information of dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZFj2skZxgpl"
      },
      "outputs": [],
      "source": [
        "# display first five rows of the dataframe - df\n",
        "print(\"First five rows:\")\n",
        "df.head(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7cA28XSx1I1"
      },
      "outputs": [],
      "source": [
        "# print the dimensions of dataframe - df\n",
        "print(\"Dimensions i.e. rows and columns\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-gsbEhJx2rm"
      },
      "outputs": [],
      "source": [
        "# print the information of the dataframe\n",
        "print(\"Information of dataframe:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y18LwoqyFpk"
      },
      "source": [
        "#### **2.2** *Recipe Data Manipulation* <font color = red>[13 marks]</font> <br>\n",
        "Create derived metrics in dataframe and provide insights of the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhNG_XC1r4Qw"
      },
      "source": [
        "##### **2.2.1** **Create input_tokens and pos_tokens columns by splitting the input and pos from the dataframe** <font color = red>[3 marks]</font> <br>\n",
        "Split the input and pos into input_tokens and pos_tokens in the dataframe and display it in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nma6uJwmXUas"
      },
      "outputs": [],
      "source": [
        "# split the input and pos into input_tokens and pos_tokens in the dataframe\n",
        "\n",
        "# Tokenize input\n",
        "df['input_tokens']=df['input'].str.split()\n",
        "\n",
        "# Tokenize POS\n",
        "df['pos_tokens']=df['pos'].str.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g-ajvFBzaaf"
      },
      "outputs": [],
      "source": [
        "# display first five rows of the dataframe - df\n",
        "print(\"Input_tokens\")\n",
        "df['input_tokens'].head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-2AjgOv4AAf"
      },
      "outputs": [],
      "source": [
        "print(\"Pos_tokens\")\n",
        "df['pos_tokens'].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JtvsBYur-oV"
      },
      "source": [
        "##### **2.2.2** **Provide the length for input_tokens and pos_tokens and validate their length** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create input_length and pos_length columns in the dataframe and validate both the lengths. Check for the rows that are unequal in input and pos length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeVRD2IK1Jrg"
      },
      "outputs": [],
      "source": [
        "# create input_length and pos_length columns for the input_tokens and pos-tokens\n",
        "df['input_length'] = df['input_tokens'].apply(len)\n",
        "df['pos_tokens']=df['pos_tokens'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPMOlLnz1P1H"
      },
      "outputs": [],
      "source": [
        "# check for the equality of input_length and pos_length in the dataframe\n",
        "df['lengths_equity'] = df['input_length'] == df['pos_tokens']\n",
        "print(df['lengths_equity'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpJQu3JE_P7Z"
      },
      "source": [
        "##### **2.2.3** **Define a unique_labels function and validate the labels in pos_tokens** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a unique_labels function which checks for all the unique pos labels in the recipe & execute it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAN0a0YX4AAj"
      },
      "outputs": [],
      "source": [
        "df['pos_tokens'] = df['pos_tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4aMFCxXO_GJ"
      },
      "outputs": [],
      "source": [
        "# Define a unique_labels function to checks for all the unique pos labels in the recipe & print it\n",
        "def unique_labels(filepath):\n",
        "  all_labels = set()\n",
        "  for pos_string in df['pos']:\n",
        "        if isinstance(pos_string, str):\n",
        "            labels = pos_string.strip().split()\n",
        "            all_labels.update(labels)\n",
        "  print(\"Unique POS labels:\")\n",
        "  print(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V8RKzeW4AAl"
      },
      "outputs": [],
      "source": [
        "unique_labels(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbriClEV9CW5"
      },
      "source": [
        "##### **2.2.3** **Provide the insights seen in the recipe data after validation** <font color = red>[1 marks]</font> <br>\n",
        "\n",
        "Provide the indexes that requires cleaning and formatting in the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrNQ4AtD9RPk"
      },
      "source": [
        "<font color = red>[write your answer]</font> <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtqtij2-CD2m"
      },
      "source": [
        "##### **2.2.4** **Drop the rows that have invalid data provided in previous cell** <font color = red> [2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaiy1pYWCFPA"
      },
      "outputs": [],
      "source": [
        "# drop the irrelevant recipe data\n",
        "rows_with_nan = df[df.isna().any(axis=1)]\n",
        "print(rows_with_nan)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLlC5YQ4AA4"
      },
      "source": [
        "# There are no null values found in the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RJEStPSC9PB"
      },
      "source": [
        "##### **2.2.5** **Update the input_length & pos_length in dataframe**<font color = red> [2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjJd7gPI5_ca"
      },
      "outputs": [],
      "source": [
        "# update the input and pos length in input_length and pos_length\n",
        "df['input_length'] = df['input_tokens'].apply(lambda x: len(x) if isinstance(x,str) else 0)\n",
        "df['pos_length'] = df['pos_tokens'].apply(lambda x: len(x) if isinstance(x,str) else 0)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJdYJ2TEDBzd"
      },
      "source": [
        "##### **2.2.6** **Validate the input_length and pos_length by checking unequal rows** <font color = red> [1 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdSsdOPM8aXo"
      },
      "outputs": [],
      "source": [
        "# validate the input length and pos length as input_length and pos_length\n",
        "unequal_rows = df[df['input_length'] != df['pos_length']]\n",
        "print(unequal_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwKLW4em-qMu"
      },
      "source": [
        "## **3** Train Validation Split (70 train - 30 val) <font color = red>[6 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_pJDTVO-71z"
      },
      "source": [
        "#### **3.1** *Perform train and validation split ratio* <font color = red>[6 marks]</font> <br>\n",
        "Split the dataset with the help of input_tokens and pos_tokens and make a ratio of 70:30 split for training and validation datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-64gdDiIy9u"
      },
      "source": [
        "###### **3.1.1** **Split the dataset into train_df and val_df into 70:30 ratio** <font color = red> [1 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W20A_-9E_WOv"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUA05_77JRAv"
      },
      "source": [
        "###### **3.1.2** **Print the first five rows of train_df and val_df** <font color = red> [1 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgMZfsbV_XhK"
      },
      "outputs": [],
      "source": [
        "# print the first five rows of train_df\n",
        "train_df.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgtg5WE4_d7h"
      },
      "outputs": [],
      "source": [
        "# print the first five rows of the val_df\n",
        "val_df.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7prEiaiqI_VZ"
      },
      "source": [
        "###### **3.1.3** **Extract the dataset into train_df and val_df into X_train, X_val, y_train and y_val and display their length** <font color = red> [2 marks]</font> <br>\n",
        "\n",
        "Extract X_train, X_val, y_train and y_val by extracting the list of input_tokens and pos_tokens from train_df and val_df and also display their length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFVnCD71IHXF"
      },
      "outputs": [],
      "source": [
        "# extract the training and validation sets by taking input_tokens and pos_tokens\n",
        "X_train = train_df['input_tokens'].tolist()\n",
        "y_train = train_df['pos_tokens'].tolist()\n",
        "\n",
        "X_val = val_df['input_tokens'].tolist()\n",
        "y_val = val_df['pos_tokens'].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQPOVz3J_fiq"
      },
      "outputs": [],
      "source": [
        "# validate the shape of training and validation samples\n",
        "print(\"Length of X_train:\", len(X_train))\n",
        "print(\"Length of y_train:\", len(y_train))\n",
        "print(\"Length of X_val:\", len(X_val))\n",
        "print(\"Length of y_val:\", len(y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uicUYglLeiA"
      },
      "source": [
        "###### **3.1.4** **Display the number of unique labels present in y_train** <font color = red> [2 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzWtzpdINt6X"
      },
      "outputs": [],
      "source": [
        "# Display the number of unique labels present in y_train\n",
        "unique_labels = set(y_train)\n",
        "print(\"Unique labels in y_train:\", unique_labels)\n",
        "print(\"Number of unique labels in y_train:\", len(unique_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFm46QrB4gmj"
      },
      "source": [
        "## **4** Exploratory Recipe Data Analysis on Training Dataset <font color = red>[16 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUWIp0n_NeH6"
      },
      "source": [
        "#### **4.1** *Flatten the lists for input_tokens & pos_tokens* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function **flatten_list** for flattening the structure for input_tokens and pos_tokens. The input parameter passed to this function is a nested list.\n",
        "\n",
        "Initialise the dataset_name with a value ***'Training'***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzcY0gPiOe8o"
      },
      "outputs": [],
      "source": [
        "# flatten the list for nested_list (input_tokens, pos_tokens)\n",
        "# Flatten input_tokens\n",
        "def flatten_list(nested_list):\n",
        "    dataset_name = \"Training\"\n",
        "\n",
        "\n",
        "    safe_list = [\n",
        "        x if isinstance(x, list) else [x]\n",
        "        for x in nested_list\n",
        "    ]\n",
        "    flattened = list(chain.from_iterable(safe_list))\n",
        "    print(f\"{dataset_name} dataset flattened length:\", len(flattened))\n",
        "    return flattened\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXRda29gNBH8"
      },
      "outputs": [],
      "source": [
        "# initialise the dataset_name\n",
        "# dataset_name = 'Training'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGOqhd8OOr1E"
      },
      "source": [
        "#### **4.2** *Extract and validate the tokens after using the flattening technique* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function named ***extract_and_validate_tokens*** with parameters dataframe and dataset_name (Training/Validation), validate the length of input_tokens and pos_tokens from dataframe and display first 10 records for both the input_tokens and pos_tokens. Execute this function\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3GMX83xP7ja"
      },
      "outputs": [],
      "source": [
        "# define a extract_and_validate_tokens with parameters (df, dataset_name)\n",
        "# call the flatten_list and apply it on input_tokens and pos_tokens\n",
        "# validate their length and display first 10 records having input and pos tokens\n",
        "def extract_and_validate_tokens(dataframe, dataset_name):\n",
        "    # Extract\n",
        "    input_tokens = dataframe['input_tokens']\n",
        "    pos_tokens = dataframe['pos_tokens']\n",
        "\n",
        "    # Validate lengths\n",
        "    mismatched_rows = []\n",
        "    for idx, (inp, pos) in enumerate(zip(input_tokens, pos_tokens)):\n",
        "        len_inp = len(inp) if isinstance(inp, list) else 1\n",
        "        len_pos = len(pos) if isinstance(pos, list) else 1\n",
        "        if len_inp != len_pos:\n",
        "            mismatched_rows.append((idx, len_inp, len_pos))\n",
        "\n",
        "    # Report validation result\n",
        "    if mismatched_rows:\n",
        "        print(f\"[{dataset_name}] Mismatched rows found:\")\n",
        "        for idx, l_inp, l_pos in mismatched_rows:\n",
        "            print(f\"Row {idx}: input_tokens length = {l_inp}, pos_tokens length = {l_pos}\")\n",
        "    else:\n",
        "        print(f\"[{dataset_name}] All rows have matching token lengths\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajdbYMgeLpf9"
      },
      "outputs": [],
      "source": [
        "# extract the tokens and its pos tags\n",
        "extract_and_validate_tokens(train_df, \"Training\")\n",
        "extract_and_validate_tokens(val_df, \"Validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htZVn5wcQSok"
      },
      "source": [
        "#### **4.3** *Categorise tokens into labels (unit, ingredient, quantity)* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define a function ***categorize_tokens*** to categorise tokens into ingredients, units and quantities by using extracted tokens in the previous code and return a list of ingredients, units and quantities. Execute this function to get the list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xq0a4L7Quct"
      },
      "outputs": [],
      "source": [
        "# define a categorize_tokens function and provide the tokens and pos_tags as parameters and create ingredient, unit and quantity list and return it\n",
        "# validate the list that it comprised of these labels, if not return empty arrays\n",
        "def categorize_tokens(input_tokens, pos_tokens):\n",
        "    # Expected POS labels\n",
        "    valid_labels = {\"ingredient\", \"unit\", \"quantity\"}\n",
        "\n",
        "    # Validation: If any tag is outside valid labels, return empty lists\n",
        "    if not all(tag in valid_labels for tag in pos_tags):\n",
        "        return [], [], []\n",
        "\n",
        "    # Initialize category lists\n",
        "    ingredients, units, quantities = [], [], []\n",
        "\n",
        "    # Categorize tokens based on POS tag\n",
        "    for token, tag in zip(input_tokens, pos_tokens):\n",
        "        if tag == \"ingredient\":\n",
        "            ingredients.append(token)\n",
        "        elif tag == \"unit\":\n",
        "            units.append(token)\n",
        "        elif tag == \"quantity\":\n",
        "            quantities.append(token)\n",
        "\n",
        "    return ingredients, units, quantities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evcsigvUL7bM"
      },
      "outputs": [],
      "source": [
        "#  call the function to categorise the labels into respective list\n",
        "ingredients, units, quantities = categorize_tokens(input_tokens, pos_tokens)\n",
        "\n",
        "print(\"Ingredients:\", ingredients)\n",
        "print(\"Units:\", units)\n",
        "print(\"Quantities:\", quantities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGau4EgZCix"
      },
      "source": [
        "#### **4.4** *Top 10 Most Frequent Items* <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Define a function ***get_top_frequent_items*** to display top 10 most frequent items\n",
        "\n",
        "Here, item_list is used as a general parameter where you will call this function for ingredient and unit list\n",
        "\n",
        "Execute this function separately for top 10 most units and ingredients\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXc8h3H4ZOZ4"
      },
      "outputs": [],
      "source": [
        "# define a function get_top_frequent_items to get the top frequent items by using item_list, pos label and dataset_name(Training/Validation) and return top items\n",
        "\n",
        "def get_top_frequent_items(df, pos_label, dataset_name, top_n=10):\n",
        "\n",
        "    items = []\n",
        "\n",
        "    for tokens, pos_tags in zip(df['input_tokens'], df['pos_tokens']):\n",
        "        # If the cell is a string like \"['apple', 'sugar']\", convert to list\n",
        "        if isinstance(tokens, str):\n",
        "            tokens = ast.literal_eval(tokens)\n",
        "        if isinstance(pos_tags, str):\n",
        "            pos_tags = ast.literal_eval(pos_tags)\n",
        "\n",
        "        # Ensure they are iterable and match length\n",
        "        if isinstance(tokens, list) and isinstance(pos_tags, list) and len(tokens) == len(pos_tags):\n",
        "            for token, pos in zip(tokens, pos_tags):\n",
        "                if pos == pos_label:\n",
        "                    items.append(token)\n",
        "\n",
        "    # Count and get top items\n",
        "    counts = Counter(items)\n",
        "    top_items = counts.most_common(top_n)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nTop {top_n} frequent '{pos_label}' items in {dataset_name} set:\")\n",
        "    for item, freq in top_items:\n",
        "        print(f\"{item}: {freq}\")\n",
        "\n",
        "    return top_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2jZCCf2MEke"
      },
      "outputs": [],
      "source": [
        "# get the top ingredients which are frequently seen in the recipe\n",
        "top_ingredients = get_top_frequent_items(train_df, \"ingredient\", \"Training\", top_n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wipghGXAMYQR"
      },
      "outputs": [],
      "source": [
        "# get the top units which are frequently seen in the recipe\n",
        "# For Training set\n",
        "top_units_train = get_top_frequent_items(train_df, \"unit\", \"Training\", top_n=10)\n",
        "\n",
        "# For Validation set\n",
        "top_units_val = get_top_frequent_items(val_df, \"unit\", \"Validation\", top_n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hldpjOHaPVZ"
      },
      "source": [
        "#### **4.5** *Plot Top 10 most frequent items* <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ImpWstybDP_"
      },
      "source": [
        "Define a function ***plot_top_items*** to plot a bar graph on top 10 most frequent items for units and ingredients\n",
        "\n",
        "Here, item_list is used as a general parameter where you will call this function for ingredient and unit list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmsq0L1vaxfc"
      },
      "outputs": [],
      "source": [
        "# define plot top items with parameters - top_item list, label to suggest whether its ingredient or unit, dataset_name\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_top_items(top_items, label, dataset_name):\n",
        "\n",
        "    # Unpack items and frequencies\n",
        "    items, counts = zip(*top_items)\n",
        "\n",
        "    # Set style\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Create barplot\n",
        "    sns.barplot(x=list(counts), y=list(items), palette=\"viridis\")\n",
        "\n",
        "    # Titles and labels\n",
        "    plt.title(f\"Top {label.capitalize()}s in {dataset_name} Dataset\", fontsize=16)\n",
        "    plt.xlabel(\"Frequency\", fontsize=14)\n",
        "    plt.ylabel(label.capitalize(), fontsize=14)\n",
        "\n",
        "    # Show plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHusCfkJ4suh"
      },
      "source": [
        "#### **4.6** *Perform EDA analysis* <font color = red>[5 marks]</font> <br>\n",
        "\n",
        "Plot the bar plots for ingredients and units and provide the insights for training dataset\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8seIqFKyYFmn"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent ingredients in training data\n",
        "plot_top_items(top_ingredients_training, label=\"ingredient\", dataset_name=\"Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbXAwiUkMtqT"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent units in training data\n",
        "\n",
        "plot_top_items(top_units_training, label=\"unit\", dataset_name=\"Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYh7zbJpCajJ"
      },
      "source": [
        "## **5** Exploratory Recipe Data Analysis on Validation Dataset (Optional)<font color = red> [0 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2wPIaOGCmk2"
      },
      "source": [
        "#### **5.1** *Execute EDA on Validation Dataset with insights (Optional)* <font color = red> [0 marks]</font> <br>\n",
        "Initialise the dataset_name as ***Validation*** and call the ***plot_top_items*** for top 10 ingredients and units in the recipe data\n",
        "Provide the insights for the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atSk0ChLPXHd"
      },
      "outputs": [],
      "source": [
        "# initialise the dataset_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFPxheIuj1o8"
      },
      "outputs": [],
      "source": [
        "# use extract and validate tokens, categorise tokens, get top frequent items for ingredient list and unit list on validation dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikwox7ccMaU8"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent ingredients in validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QjVeMWpPwKO"
      },
      "outputs": [],
      "source": [
        "# plot the top frequent units in training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvE92ait9GIS"
      },
      "source": [
        "## **6** Feature Extraction For CRF Model <font color = red>[30 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5Q_Lj09GIT"
      },
      "source": [
        "### **6.1** *Define a feature functions to take each token from recipe* <font color = red>[10 marks]</font>\n",
        "\n",
        "Define a function as ***word2features*** which takes a particular recipe and its index to work with all recipe input tokens and include custom key-value pairs.\n",
        "\n",
        "Also, use feature key-value pairs to mark the beginning and end of the sequence and to also check whether the word belongs to unit, quantity etc. Use keyword sets for unit and quantity for differentiating feature functions well. Also make use of relevant regex patterns on fractions, whole numbers etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyxmQ0PrhBra"
      },
      "source": [
        "##### **6.1.1** **Define keywords for unit and quantity and create a quantity pattern to work on fractions, numbers and decimals** <font color = red>[3 marks]</font> <br>\n",
        "\n",
        "Create sets for **unit_keywords** and ***quantity_keywords*** and include all the words relevant for measuring the ingredients such as cup, tbsp, tsp etc. and in quantity keywords, include words such as half, quarter etc.\n",
        "\n",
        "Also suggested to use regex pattern as ***quantity_pattern*** to work with quantity in any format such as fractions, numbers and decimals.\n",
        "\n",
        "Then, load the spacy model and process the entire sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhFUPxeth0KI"
      },
      "outputs": [],
      "source": [
        "unit_keywords = [\n",
        "    'g', 'gram', 'grams',\n",
        "    'kg', 'kilogram', 'kilograms',\n",
        "    'mg', 'milligram', 'milligrams',\n",
        "    'ml', 'millilitre', 'millilitres',\n",
        "    'l', 'litre', 'litres',\n",
        "    'cup', 'cups',\n",
        "    'tbsp', 'tablespoon', 'tablespoons',\n",
        "    'tsp', 'teaspoon', 'teaspoons',\n",
        "    'oz', 'ounce', 'ounces',\n",
        "    'lb', 'pound', 'pounds',\n",
        "    'pinch', 'dash', 'drop', 'clove', 'cloves',\n",
        "    'slice', 'slices', 'piece', 'pieces'\n",
        "    ]\n",
        "\n",
        "\n",
        "quantity_keywords = [\n",
        "    'few', 'some', 'several', 'handful',\n",
        "    'dozen', 'half', 'quarter',\n",
        "    'couple', 'pinch', 'dash'\n",
        "]\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "quantity_pattern = re.compile(\n",
        "    r'(\\d+(\\.\\d+)?(/\\d+)?)|(\\d+\\s*-\\s*\\d+)|(\\d+\\s*\\d+/\\d+)|(\\d+/\\d+)'  #\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qmM8rw4VtJh"
      },
      "outputs": [],
      "source": [
        "# load spaCy model\n",
        "import spacy\n",
        "\n",
        "# Load a small English model (common choice)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrYD5tMNiFc-"
      },
      "source": [
        "##### **6.1.2** **Define feature functions for CRF** <font color = red>[7 marks]</font> <br>\n",
        "\n",
        "Define ***word2features*** function and use the parameters such as sentence and its indexing as ***sent*** and ***i*** for extracting token level features for CRF Training.\n",
        "Build ***features*** dictionary, also mark the beginning and end of the sequence and use the ***unit_keywords***, ***quantity_keywords*** and ***quantity_pattern*** for knowing the presence of quantity or unit in the tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAjf6j-dQtpr"
      },
      "source": [
        "While building ***features*** dictionary, include\n",
        "- ***Core Features*** - The core features of a token should capture its lexical\n",
        "and grammatical properties. Include attributes like the raw token, its lemma, part-of-speech tag, dependency relation, and shape, as well as indicators for whether it's a stop word, digit, or punctuation. The details of the features are given below:\n",
        "\n",
        "    - `bias` - Constant feature with a fixed value of 1.0 to aid model learning.\n",
        "    - `token` - The lowercase form of the current token.\n",
        "    - `lemma` - The lowercase lemma (base form) of the token.\n",
        "    - `pos_tag` - Part-of-speech (POS) tag of the token.\n",
        "    - `tag` - Detailed POS tag of the token.\n",
        "    - `dep` - Dependency relation of the token in the sentence.\n",
        "    - `shape` - Shape of the token (e.g., \"Xxx\" for \"Milk\").\n",
        "    - `is_stop` - Boolean indicating if the token is a stopword.\n",
        "    - `is_digit` - Boolean indicating if the token consists of only digits.\n",
        "    - `has_digit` - Boolean indicating if the token contains at least one digit.\n",
        "    - `has_alpha` - Boolean indicating if the token contains at least one alphabetic character.\n",
        "    - `hyphenated` - Boolean indicating if the token contains a hyphen (-).\n",
        "    - `slash_present` - Boolean indicating if the token contains a slash (/).\n",
        "    - `is_title` - Boolean indicating if the token starts with an uppercase letter.\n",
        "    - `is_upper` - Boolean indicating if the token is fully uppercase.\n",
        "    - `is_punct` - Boolean indicating if the token is a punctuation mark.\n",
        "\n",
        "- ***Improved Quantity and Unit Detection*** - Use key-value pairs to mark the presence of quantities and units in the features dictionary. Utilise the unit_keywords, quantity_keywords, and quantity_pattern to identify and flag these elements. The details of the features are given below:\n",
        "\n",
        "    - `is_quantity` - Boolean indicating if the token matches a quantity pattern or keyword.\n",
        "    - `is_unit` - Boolean indicating if the token is a known measurement unit.\n",
        "    - `is_numeric` - Boolean indicating if the token matches a numeric pattern.\n",
        "    - `is_fraction` - Boolean indicating if the token represents a fraction (e.g., 1/2).\n",
        "    - `is_decimal` - Boolean indicating if the token represents a decimal number (e.g., 3.14).\n",
        "    - `preceding_word` - The previous token in the sentence, if available.\n",
        "    - `following_word` - The next token in the sentence, if available.\n",
        "\n",
        "- ***Contextual Features*** - Incorporate contextual information by adding features for the preceding and following tokens. Include indicators like BOS and EOS to mark the beginning and end of the sequence, and utilise unit_keywords, quantity_keywords, and quantity_pattern to identify the types of neighboring tokens. The features are given below:\n",
        "\n",
        "    - `prev_token` - The lowercase form of the previous token.\n",
        "    - `prev_is_quantity` - Boolean indicating if the previous token is a quantity.\n",
        "    - `prev_is_digit` - Boolean indicating if the previous token is a digit.\n",
        "    - `BOS` - Boolean indicating if the token is at the beginning of the sentence.\n",
        "    - `next_token` - The lowercase form of the next token.\n",
        "    - `next_is_unit` - Boolean indicating if the next token is a unit.\n",
        "    - `next_is_ingredient` - Boolean indicating if the next token is not a unit or quantity.\n",
        "    - `EOS` - Boolean indicating if the token is at the end of the sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRU7efTF9GIW"
      },
      "outputs": [],
      "source": [
        "# define word2features for processing each token in the sentence sent by using index i.\n",
        "# use your own feature functions\n",
        "\n",
        "    # Process the entire sentence with spaCy\n",
        "\n",
        "    # --- Core Features ---\n",
        "\n",
        "    # --- Improved Quantity & Unit Detection ---\n",
        "\n",
        "    # --- Contextual Features ---\n",
        "def word2features(sent, i):\n",
        "    token = sent[i]\n",
        "    features = {}\n",
        "\n",
        "    # Core Features\n",
        "    features[\"bias\"] = 1.0\n",
        "    features[\"word.lower()\"] = token.text.lower()\n",
        "    features[\"word.isupper()\"] = token.is_upper\n",
        "    features[\"word.istitle()\"] = token.is_title\n",
        "    features[\"word.isdigit()\"] = token.is_digit\n",
        "    features[\"pos\"] = token.pos_\n",
        "    features[\"tag\"] = token.tag_\n",
        "    features[\"dep\"] = token.dep_\n",
        "    features[\"shape\"] = token.shape_\n",
        "    features[\"lemma\"] = token.lemma_.lower()\n",
        "\n",
        "    # Improved Quantity & Unit Detection\n",
        "    features[\"is_unit_keyword\"] = token.text.lower() in unit_keywords\n",
        "    features[\"is_quantity_keyword\"] = token.text.lower() in quantity_keywords\n",
        "    features[\"matches_quantity_pattern\"] = bool(quantity_pattern.match(token.text))\n",
        "\n",
        "    # Contextual Features\n",
        "    if i > 0:\n",
        "        token_prev = sent[i - 1]\n",
        "        features[\"-1:word.lower()\"] = token_prev.text.lower()\n",
        "        features[\"-1:pos\"] = token_prev.pos_\n",
        "    else:\n",
        "        features[\"BOS\"] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        token_next = sent[i + 1]\n",
        "        features[\"+1:word.lower()\"] = token_next.text.lower()\n",
        "        features[\"+1:pos\"] = token_next.pos_\n",
        "    else:\n",
        "        features[\"EOS\"] = True\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJm2nUw0998s"
      },
      "source": [
        "### **6.2** *Preparation of Recipe level features* <font color = red>[2 marks]</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL19ooQejA5z"
      },
      "source": [
        "##### **6.2.1** **Define function to work on all the recipes and call word2features for each recipe** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Define ***sent2features*** function and inputs ***sent*** as a parameter and correctly generate feature functions for each token present in the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQEifz-9GIW"
      },
      "outputs": [],
      "source": [
        "# define sent2features by working on each token in the sentence and correctly generate dictionaries for features\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOK0t3c6-RiV"
      },
      "source": [
        "### **6.3** *Convert X_train, X_val, y_train and y_val into train and validation feature sets and labels* <font color = red>[6 marks]</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tsd50b_nX0J"
      },
      "source": [
        "##### **6.3.1** **Convert recipe into feature functions by using X_train and X_val** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***X_train_features*** and ***X_val_features*** as list to include the feature functions for each recipe present in training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-bVPGPa39GIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "c2d2f56d-90b5-4fb3-aa38-b2a5669cf1b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1780610298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert input sentences into feature sets by taking training and validation dataset as X_train_features and X_val_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_val_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Convert input sentences into feature sets by taking training and validation dataset as X_train_features and X_val_features\n",
        "X_train_features = [sent2features(sent) for sent in X_train]\n",
        "X_val_features = [sent2features(sent) for sent in X_val]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcwmwXn-n6cs"
      },
      "source": [
        "##### **6.3.2** **Convert lables of y_train and y_val into list** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***y_train_labels*** and ***y_val_labels*** by using the list of y_train and y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TiGgP3O6nfPg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "ffce4827-f611-4f58-d1ea-23ccb852bf69"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3558962998.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert labels into list as y_train_labels and y_val_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_val_labels\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Convert labels into list as y_train_labels and y_val_labels\n",
        "y_train_labels = [list(labels) for labels in df_train['pos_tokens']]\n",
        "y_val_labels   = [list(labels) for labels in df_val['pos_tokens']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c-kjqtaoZvb"
      },
      "source": [
        "##### **6.3.3** **Print the length of val and train features and labels** <font color = red>[2 marks]</font> <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mWId2Nn0okMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "19145677-13b5-4cd9-eff3-46244692a63e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1662982078.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print the length of train features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of X_train_features:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of y_train_labels:  \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_features' is not defined"
          ]
        }
      ],
      "source": [
        "# print the length of train features and labels\n",
        "print(\"Length of X_train_features:\", len(X_train_features))\n",
        "print(\"Length of y_train_labels:  \", len(y_train_labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAt_m_LubRvn"
      },
      "outputs": [],
      "source": [
        "# print the length of validation features and labels\n",
        "print(\"Length of X_val_features:  \", len(X_val_features))\n",
        "print(\"Length of y_val_labels:    \", len(y_val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZffFBH-pVhx"
      },
      "source": [
        "### **6.4** *Applying weights to feature sets* <font color = red>[12 marks]</font> <br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goh_fX-6pqhN"
      },
      "source": [
        "##### **6.4.1** **Flatten the labels of y_train** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***y_train_flat*** to flatten the structure of nested y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "adLWfYn_p3gM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "dd669841-bae6-4b36-89cc-a513a8ca1d3c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_train_labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-806692185.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Flatten labels in y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train_labels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train_labels' is not defined"
          ]
        }
      ],
      "source": [
        "# Flatten labels in y_train\n",
        "y_train_flat = [label for sentence_labels in y_train_labels for label in sentence_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk9UMBrbp9dp"
      },
      "source": [
        "##### **6.4.2** **Count the labels present in training target dataset** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***label_counts*** to count the frequencies of labels present in y_train_flat and retrieve the total samples by using the values of label_counts as ***total_samples***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6Kiu8jckqZSH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "43952596-e003-4e3c-8b9c-a2fd1a3bb530"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Counter' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-444071996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Count label frequencies as label_counts and total_samples as getting the summation of values of label_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label Counts:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total Samples:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
          ]
        }
      ],
      "source": [
        "# Count label frequencies as label_counts and total_samples as getting the summation of values of label_counts\n",
        "label_counts = Counter(y_train_flat)\n",
        "total_samples = sum(label_counts.values())\n",
        "print(\"Label Counts:\", label_counts)\n",
        "print(\"Total Samples:\", total_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aCmDsZYqYA-"
      },
      "source": [
        "##### **6.4.3** **Compute weight_dict by using inverse frequency method for label weights** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "- Create ***weight_dict*** as dictionary with label and its inverse frequency count in ***label_counts***\n",
        "\n",
        "- Penalise ingredient label in the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FpbEAZ3zqxEo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "dbd2e6bc-5b40-4f36-8bfd-ea26ffd68d9c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'label_counts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3008674539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m class_weights = {\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'label_counts' is not defined"
          ]
        }
      ],
      "source": [
        "# Compute class weights (inverse frequency method) by considering total_samples and label_counts\n",
        "class_weights = {\n",
        "    label: total_samples / (len(label_counts) * count)\n",
        "    for label, count in label_counts.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hns3HbujXESs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "3ff4af49-39db-4ce8-ca32-d174a904c68f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'class_weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1199019789.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpenalty_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m'ingredient'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mclass_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ingredient'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mpenalty_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'class_weights' is not defined"
          ]
        }
      ],
      "source": [
        "# penalise ingredient label\n",
        "penalty_factor = 2.0\n",
        "\n",
        "if 'ingredient' in class_weights:\n",
        "    class_weights['ingredient'] *= penalty_factor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8TdHMlPrhh8"
      },
      "source": [
        "##### **6.4.4** **Extract features along with class weights** <font color = red>[4 marks]</font> <br>\n",
        "\n",
        "Define a function ***extract_features_with_class_weights*** to work with training and validation datasets and extract features by applying class weights\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1km6GR4TjXPX"
      },
      "outputs": [],
      "source": [
        "# Apply weights to feature extraction in extract_features_with_class_weights by using parameters such as X (input tokens), y(labels) and weight_dict (Class weights)\n",
        "def extract_features_with_class_weights(X, y, weight_dict):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for tokens, labels_seq in zip(X, y):\n",
        "        for token, label in zip(tokens, labels_seq):\n",
        "            # Base feature extraction\n",
        "            feat = {\n",
        "                'token': token,\n",
        "                'is_digit': token.isdigit(),\n",
        "                'is_alpha': token.isalpha(),\n",
        "                'token_length': len(token)\n",
        "            }\n",
        "\n",
        "            # Apply weight  store it as an additional feature\n",
        "            feat['class_weight'] = weight_dict.get(label, 1.0)\n",
        "\n",
        "            features.append(feat)\n",
        "            labels.append(label)\n",
        "\n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51ABmKwKsaiz"
      },
      "source": [
        "##### **6.4.5** **Execute extract_features_with_class_weights on training and validation datasets** <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "Create ***X_train_weighted_features*** and ***X_val_weighted_features*** for extracting training and validation features along with their weights by calling ***extract_features_with_class_weights*** on the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-XUFFnm5sYE6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "10c2d52d-657f-4d49-8d96-1cfd4eb11240"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3862208944.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Extract weighted features for training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m X_train_weighted_features, y_train_flat, train_sample_weights = extract_features_with_class_weights(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX_train_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0my_train_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mweight_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_tokens' is not defined"
          ]
        }
      ],
      "source": [
        "# Apply manually computed class weights\n",
        "# Extract weighted features for training set\n",
        "X_train_weighted_features, y_train_flat, train_sample_weights = extract_features_with_class_weights(\n",
        "    X_train_tokens,\n",
        "    y_train_labels,\n",
        "    weight_dict\n",
        ")\n",
        "\n",
        "# Extract weighted features for validation set\n",
        "X_val_weighted_features, y_val_flat, val_sample_weights = extract_features_with_class_weights(\n",
        "    X_val_tokens,\n",
        "    y_val_labels,\n",
        "    weight_dict\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aah9bFDlAuzI"
      },
      "source": [
        "## **7** Model Building and Training <font color = red>[10 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axrvWR9TAuzJ"
      },
      "source": [
        "### **7.1** *Initialise the CRF model and train it* <font color = red>[5 marks]</font>\n",
        "Train the CRF model with the specified hyperparameters such as\n",
        "\n",
        "### CRF Model Hyperparameters Explanation\n",
        "\n",
        "| Parameter                  | Description |\n",
        "|----------------------------|-------------|\n",
        "| **algorithm='lbfgs'**      | Optimisation algorithm used for training. `lbfgs` (Limited-memory BroydenFletcherGoldfarbShanno) is a quasi-Newton optimisation method. |\n",
        "| **c1=0.5**                | L1 regularisation term to control sparsity in feature weights. Helps in feature selection. |\n",
        "| **c2=1.0**                | L2 regularisation term to prevent overfitting by penalising large weights. |\n",
        "| **max_iterations=100**     | Maximum number of iterations for model training. Higher values allow more convergence but increase computation time. |\n",
        "| **all_possible_transitions=True** | Ensures that all possible state transitions are considered in training, making the model more robust. |\n",
        "\n",
        "Use weight_dict for training CRF\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jig2J_n1AuzM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "cfc988e7-e481-4dca-9fe5-8e78891108ef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sklearn_crfsuite' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4046155059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialise CRF model with the specified hyperparameters and use weight_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m crf = sklearn_crfsuite.CRF(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;31m# Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;31m# L1 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;31m# L2 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sklearn_crfsuite' is not defined"
          ]
        }
      ],
      "source": [
        "# initialise CRF model with the specified hyperparameters and use weight_dict\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',            # Optimizer\n",
        "    c1=0.1,                       # L1 regularization\n",
        "    c2=0.1,                       # L2 regularization\n",
        "    max_iterations=100,           # Iterations\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "# train the CRF model with the weighted training data\n",
        "crf.fit(\n",
        "    X_train_weighted_features,    # List of dicts per token\n",
        "    y_train_flat,                 # Labels\n",
        "    sample_weight=train_sample_weights\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDLwvYqOF6m_"
      },
      "source": [
        "### **7.2** *Evaluation of Training Dataset using CRF model* <font color = red>[4 marks]</font>\n",
        "Evaluate on training dataset using CRF by using flat classification report and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Us57jWSQ6laL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0b97940c-d8e0-4a5c-b907-d49e859ab9fd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'crf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3332361037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate on the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Predict on training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_weighted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mflat_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training Accuracy: {flat_accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'crf' is not defined"
          ]
        }
      ],
      "source": [
        "# evaluate on the training dataset\n",
        "# Predict on training set\n",
        "y_train_pred = crf.predict(X_train_weighted_features)\n",
        "flat_accuracy = metrics.flat_accuracy_score(y_train_labels, y_train_pred)\n",
        "print(f\"Training Accuracy: {flat_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gNGZnd-D6oq3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "17b8c317-0f88-47ba-f15d-37d7f033eda5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'crf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13155562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# specify the flat classification report by using training data for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_weighted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mflat_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Flat Accuracy on Training Data: {flat_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'crf' is not defined"
          ]
        }
      ],
      "source": [
        "# specify the flat classification report by using training data for evaluation\n",
        "y_train_pred = crf.predict(X_train_weighted_features)\n",
        "flat_acc = metrics.flat_accuracy_score(y_train_labels, y_train_pred)\n",
        "print(f\"Flat Accuracy on Training Data: {flat_acc:.4f}\")\n",
        "\n",
        "# Generate detailed flat classification report\n",
        "report = metrics.flat_classification_report(\n",
        "    y_train_labels,\n",
        "    y_train_pred,\n",
        "    digits=3\n",
        ")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "metadata": {
        "id": "GttZgoA9bmPy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GqP9WBvJ63qm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e55096ae-a6bd-4e5d-a97d-eb6416f56f2f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_train_labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2461162304.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a confusion matrix on training datset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_true_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train_labels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train_pred\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_train_labels' is not defined"
          ]
        }
      ],
      "source": [
        "# create a confusion matrix on training datset\n",
        "y_true_flat = [label for seq in y_train_labels for label in seq]\n",
        "y_pred_flat = [label for seq in y_train_pred for label in seq]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=list(weight_dict.keys()))\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(weight_dict.keys()))\n",
        "disp.plot(cmap='Blues', xticks_rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yps2-XscGuHc"
      },
      "source": [
        "### **7.3** *Save the CRF model* <font color = red>[1 marks]</font>\n",
        "Save the CRF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iAYDLatcGzEN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "68049f97-eabe-4625-ac9f-660be6b17788"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'crf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1579715014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dump the model using joblib as crf_model.pkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"crf_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CRF model saved as crf_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'crf' is not defined"
          ]
        }
      ],
      "source": [
        "# dump the model using joblib as crf_model.pkl\n",
        "import joblib\n",
        "joblib.dump(crf, \"crf_model.pkl\")\n",
        "\n",
        "print(\"CRF model saved as crf_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agM32oUlBo1K"
      },
      "source": [
        "## **8** Prediction and Model Evaluation <font color = red>[3 marks]</font> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5BYmkTrBo1L"
      },
      "source": [
        "### **8.1** *Predict and Evaluate the CRF model on validation set* <font color = red>[3 marks]</font>\n",
        "Evaluate the metrics for CRF model by using flat classification report and confusion matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qhH6Sp8tBo1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0da9f203-5562-409b-8f93-0cb4162a0178"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'crf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1900885337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# predict the crf model on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_val_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_weighted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{token:15} True: {true_label:10} Pred: {pred_label}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'crf' is not defined"
          ]
        }
      ],
      "source": [
        "# predict the crf model on validation dataset\n",
        "y_val_pred = crf.predict(X_val_weighted_features)\n",
        "for token, true_label, pred_label in zip(X_val_tokens[0], y_val_labels[0], y_val_pred[0]):\n",
        "    print(f\"{token:15} True: {true_label:10} Pred: {pred_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "SMktt_w1kovB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "7106b13a-4827-4f62-a6a6-27d6602421bf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'metrics' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1049962461.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# specify flat classification report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mflat_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_accuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Flat Accuracy on Training Data: {flat_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m val_report = metrics.flat_classification_report(\n\u001b[1;32m      5\u001b[0m     \u001b[0my_val_labels\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# True labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
          ]
        }
      ],
      "source": [
        "# specify flat classification report\n",
        "flat_acc = metrics.flat_accuracy_score(y_train_labels, y_train_pred)\n",
        "print(f\"Flat Accuracy on Training Data: {flat_acc:.4f}\")\n",
        "val_report = metrics.flat_classification_report(\n",
        "    y_val_labels,    # True labels\n",
        "    y_val_pred,      # Predicted labels\n",
        "    digits=3\n",
        ")\n",
        "\n",
        "print(\"Flat Classification Report (Validation Data):\")\n",
        "print(val_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eI2tUBRRk4jK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f0dc92a4-e370-4309-c71b-2e1baddaae98"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_val_labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3837482208.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a confusion matrix on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_val_true_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_val_labels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_val_pred_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_val_pred\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_val_labels' is not defined"
          ]
        }
      ],
      "source": [
        "# create a confusion matrix on validation dataset\n",
        "y_val_true_flat = [label for seq in y_val_labels for label in seq]\n",
        "y_val_pred_flat = [label for seq in y_val_pred for label in seq]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm_val = confusion_matrix(\n",
        "    y_val_true_flat,\n",
        "    y_val_pred_flat,\n",
        "    labels=list(weight_dict.keys())  # Ensure consistent class order\n",
        ")\n",
        "\n",
        "# Display confusion matrix\n",
        "disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=list(weight_dict.keys()))\n",
        "disp_val.plot(cmap='Blues', xticks_rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pD6hD3NEV3q"
      },
      "source": [
        "## **9** Error Analysis on Validation Data <font color = red>[10 marks]</font> <br>\n",
        "Investigate misclassified samples in validation dataset and provide the insights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9tUvjrzFjib"
      },
      "source": [
        "### **9.1** *Investigate misclassified samples in validation dataset* <font color = red>[8 marks]</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb15uObqxKe4"
      },
      "source": [
        "##### **9.1.1** Flatten the labels of validation data and initialise error data <font color = red>[2 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Flatten the true and predicted labels and initialise the error data as ***error_data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbgYAjd-UzkI"
      },
      "outputs": [],
      "source": [
        "# y_val_true_flat = [label for seq in y_val_labels for label in seq]\n",
        "y_val_pred_flat = [label for seq in y_val_pred for label in seq]\n",
        "\n",
        "# Initialise error data list\n",
        "error_data = []\n",
        "\n",
        "# Collect mismatches\n",
        "for true_label, pred_label in zip(y_val_true_flat, y_val_pred_flat):\n",
        "    if true_label != pred_label:\n",
        "        error_data.append({\n",
        "            \"true_label\": true_label,\n",
        "            \"pred_label\": pred_label\n",
        "        })\n",
        "\n",
        "print(f\"Total errors found: {len(error_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS9foWfdXHOg"
      },
      "source": [
        "##### **9.1.2** Iterate the validation data and collect Error Information<font color = red> [2 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Iterate through validation data (X_val, y_val_labels, y_pred_val) and compare true vs. predicted labels. Collect error details, including surrounding context, previous/next tokens, and class weights, then store them in error_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VKLc1s0U0yY"
      },
      "outputs": [],
      "source": [
        "# iterate and collect Error Information\n",
        "\n",
        "\n",
        "# Iterate over each validation sequence\n",
        "for tokens, true_seq, pred_seq in zip(X_val_tokens, y_val_labels, y_val_pred):\n",
        "    for token, true_label, pred_label in zip(tokens, true_seq, pred_seq):\n",
        "        if true_label != pred_label:\n",
        "            error_data.append({\n",
        "                \"token\": token,\n",
        "                \"true_label\": true_label,\n",
        "                \"pred_label\": pred_label\n",
        "            })\n",
        "\n",
        "print(f\"Total misclassified tokens: {len(error_data)}\")\n",
        "\n",
        "            # get previous and next tokens with handling for boundary cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_R8CCAFZSzF"
      },
      "source": [
        "##### **9.1.3** Create dataframe from error_data and print overall accuracy <font color = red>[1 marks]</font> <br>\n",
        "\n",
        "\n",
        "\n",
        "Change error_data into dataframe and then use it to illustrate the overall accuracy of validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUffRP7XU3YC"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame and Print Overall Accuracy\n",
        "for tokens, true_seq, pred_seq in zip(X_val_tokens, y_val_labels, y_val_pred):\n",
        "    for i, (token, true_label, pred_label) in enumerate(zip(tokens, true_seq, pred_seq)):\n",
        "        if true_label != pred_label:\n",
        "            error_data.append({\n",
        "                \"prev_token\": tokens[i-1] if i > 0 else None,         # None if first token\n",
        "                \"token\": token,\n",
        "                \"next_token\": tokens[i+1] if i < len(tokens)-1 else None,  # None if last token\n",
        "                \"true_label\": true_label,\n",
        "                \"pred_label\": pred_label\n",
        "            })\n",
        "\n",
        "print(f\"Total misclassified tokens: {len(error_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OUYHFmgZhgJ"
      },
      "source": [
        "##### **9.1.4** Analyse errors by label type<font color = red> [3 marks]</font> <br>\n",
        "Analyse errors found in the validation data by each label and display their class weights along with accuracy and also display the error dataframe with token,  previous token, next token, true label, predicted label and context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "zu8CtjU6WR9l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "66e53738-bb9e-4874-ff44-bfc9ac3fab13"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_val_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1714955548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and display their class weights along with accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# and display the error dataframe with token, previous token, next token, true label, predicted label and context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrue_label\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_val_tokens' is not defined"
          ]
        }
      ],
      "source": [
        "# Analyse errors found in the validation data by each label\n",
        "# and display their class weights along with accuracy\n",
        "# and display the error dataframe with token, previous token, next token, true label, predicted label and context\n",
        "for tokens, true_seq, pred_seq in zip(X_val_tokens, y_val_labels, y_val_pred):\n",
        "    for i, (token, true_label, pred_label) in enumerate(zip(tokens, true_seq, pred_seq)):\n",
        "        if true_label != pred_label:\n",
        "            error_data.append({\n",
        "                \"prev_token\": tokens[i-1] if i > 0 else None,\n",
        "                \"token\": token,\n",
        "                \"next_token\": tokens[i+1] if i < len(tokens)-1 else None,\n",
        "                \"true_label\": true_label,\n",
        "                \"pred_label\": pred_label,\n",
        "                \"context\": \" \".join(tokens[max(0, i-2): min(len(tokens), i+3)])  # 2 tokens before & after\n",
        "            })\n",
        "\n",
        "error_df = pd.DataFrame(error_data)\n",
        "y_val_true_flat = [label for seq in y_val_labels for label in seq]\n",
        "y_val_pred_flat = [label for seq in y_val_pred for label in seq]\n",
        "\n",
        "label_counts = Counter(y_val_true_flat)\n",
        "label_errors = Counter(error[\"true_label\"] for error in error_data)\n",
        "\n",
        "analysis_data = []\n",
        "for label in label_counts:\n",
        "    total = label_counts[label]\n",
        "    errors = label_errors.get(label, 0)\n",
        "    accuracy = (total - errors) / total if total > 0 else 0\n",
        "    analysis_data.append({\n",
        "        \"label\": label,\n",
        "        \"class_weight\": weight_dict.get(label, 1.0),\n",
        "        \"total_tokens\": total,\n",
        "        \"errors\": errors,\n",
        "        \"accuracy\": round(accuracy, 3)\n",
        "    })\n",
        "\n",
        "analysis_df = pd.DataFrame(analysis_data).sort_values(by=\"accuracy\", ascending=True)\n",
        "print(\"Error Analysis by Label:\")\n",
        "print(analysis_df)\n",
        "\n",
        "print(\"\\nError DataFrame (first 10 rows):\")\n",
        "print(error_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3n74kVvEV3q"
      },
      "source": [
        "### **9.2** *Provide insights from the validation dataset* <font color = red>[2 marks]</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Units misread as Ingredients: Words like \"cup\" in \"1 cup sugar\" can be predicted as \"ingredient\" if model sees unusual contexts.\n",
        "\n",
        "Ingredients split incorrectly: Multi-word ingredients (e.g., \"olive oil\") sometimes have one word mislabeled due to preceding punctuation or quantities.\n",
        "\n",
        "Boundary Errors: First and last tokens in sequences occasionally misclassified, likely due to lack of surrounding context."
      ],
      "metadata": {
        "id": "oSmzo3cBtfbW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWZdf1O_vWnD"
      },
      "source": [
        "[link text](https://) <font color = red>[Write your answer]</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUjFPBMxH20n"
      },
      "source": [
        "## **10** Conclusion (Optional) <font color = red>[0 marks]</font> <br>\n",
        "\n",
        "Write your findings and conclusion."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-ksMVNgeyiLN",
        "1y18LwoqyFpk",
        "bhNG_XC1r4Qw",
        "-JtvsBYur-oV",
        "dpJQu3JE_P7Z",
        "JbriClEV9CW5",
        "Qtqtij2-CD2m",
        "_RJEStPSC9PB",
        "qJdYJ2TEDBzd",
        "hJm2nUw0998s"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}